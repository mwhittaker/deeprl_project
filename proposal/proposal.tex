\documentclass{report}
\title{Deep RL Project Proposal}
\author{Vlad Feinberg, Samvit Jain, Michael Whittaker}
\date{October 9, 2017}

\usepackage{pervasives}
\usepackage{indentfirst}

\begin{document}
\maketitle{}

\section*{Overview}
Consider trying to build a policy to play the Atari game Pong by training it
with raw images of the game (like we did in homework 3). The raw images are
very high-dimensional and contain a lot of superfluous information. The only
``pertinent information'' in the images is the position and velocity of the
ball and the position of the paddles. If we instead trained a model exclusively
on this pertinent information (instead of the raw images), the model would
likely train much faster. In this project, we plan on exploring the hypothesis
that (a) we can automatically infer this pertinent information in the context of
model-based reinforcement learning by learning an encoding from the observation
space to a smaller space that is more amenable to learning and (b) policies
trained on the encoded space converge to a good policy faster than when trained
on the unencoded space.

\section*{Game Plan}
More concretely, our project will explore algorithms like the following which
jointly learn an encoding of the observation space and the dynamics.
\begin{enumerate}
  \item
    Generate a set of $N$ samples $\set{(s_i, a_i, s_i')}$ from policy $\pi$.
  \item
    Train an autoencoder $(e, d)$ and dynamics $f$ jointly minimizing
    $\sum_{i=1}^N \norm{e(d(s_i)) - s_i}_2 + \alpha \norm{f(e(s_i), a_i) -
    e(s_i')}_2$.
  \item
    Train a policy $\pi$ on $e$-encoded states using some reinforcement
    learning algorithm
  \item
    Goto 1.
\end{enumerate}
We plan on approaching the problem incrementally in the following way:
\begin{itemize}
  \item
    Train a model on a Pong simulator which directly exposes the position and
    velocity of the ball and the position of the paddles. Verify that it is
    significantly easier to train a good model on this simulator compared to
    one which only exposes raw images.
  \item
    Hard code a dynamics $f$ that is defined in terms of ball and paddle
    positions and velocities and learn the autoencoder $(e, d)$.
  \item
    Jointly learn the autoencoder $(e, d)$ and the dynamics $f$.
  \item
    Apply the technique to other problems, besides Pong.
  \item
    Compare the performance of our approach to other existing approaches.
\end{itemize}

\section*{Related Work}
\cite{munk2016learning} In this paper, states are encoded with the following loss: $|f(e(s)) - e(s')|$ (i.e. only the prediction error). With this method, the authors are not able to achieve significant improvements over the baseline performance. We believe this is partially because a loss function that does not include a term that explicitly captures the encoding error allows for degenerate solutions of the form $e(\cdot) = d(\cdot) = 1$. \newline %TODO: do we have enough evidence that this degenerate solution is an actual issue here?

\cite{watter2015embed} In this paper, the authors use locally linear dynamics on the latent space optimized jointly with encoding and decoding. They then solve a stochastic optimal control problem in latent space with iLQR to get an action plan. This is a pretty good use of the learned model; however, we believe that in practice it may be difficult to find environments where there are no locally linear dynamics. \newline %TODO: elaborate on last idea?

\cite{shelhamer2016loss} In this paper, the authors consider predicting rewards, dynamics, and which action was taken for a pair of states both in a pre-training setting and by jointly optimizing with auxiliary losses. Both are pretty effective. The main difference between their pretraining and our proposed algorithm is that they don't have a step 4 so pre-training only helps at the beginning of the optimization. You can say that the auxiliary loss formulation in this paper is the same as our algorithm, but with two key differences: 1) weight sharing between $e$, $f$, and $\pi$, and 2) the concurrent execution of steps 2 and 3. The other important difference is the way they predict dynamics. \newline %TODO: elaborate on last idea?

\cite{venkatraman2017predictive} Letting $f$ be an RNN, the authors use it to predict statistics about future observations, i.e. $|d(f(e(s), a)) - s'|$. Though they achieve modest improvements over the baseline, this may have the same problem as \cite{munk2016learning}, i.e. the lack of a term capturing encoding error may lead to degenerate encodings.

\bibliographystyle{plain}
\bibliography{citations}
\end{document}
