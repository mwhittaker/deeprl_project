\documentclass{report}
\title{Deep RL Project Proposal}
\author{Vlad Feinberg, Samvit Jain, Michael Whittaker}
\date{October 9, 2017}

\usepackage{pervasives}
\usepackage{indentfirst}
\input{defs}

\begin{document}
\maketitle{}

\section*{Overview}

\textbf{Motivating Example}. Consider trying to build a policy to play the Atari
game Pong by training it
with raw images of the game. The raw images are
very high-dimensional and contain a lot of superfluous information. The only
``pertinent information'' in the images is the position and velocity of the
ball and the position of the paddles. Based only on this information, a Pong
policy would be pretty simple (move the paddle to the expected location for
the ball).

\textbf{Hypothesis}.
We would like to investigate to what extent predictive state representations (PSRs)
are feasible to attain and useful for the implicit planning done by model-free
policies, and, if time
permits, the explicit planning of model-based ones. This requires evaluating
the hypothesis
that (a) we can automatically infer PSRs and dynamics and (b) policies
trained on PSRs converge faster than when trained
on the unencoded space. We will only consider settings with known reward $r$.

\section*{Game Plan}
We will consider algorithms which repeat the following steps:
\begin{enumerate}
  \item
    Generate on-policy samples $\mcD=\ca{(s, a, s')}$ from policy $\pi$.
  \item
    Train an autoencoder $(e, d)$ and dynamics $f$ by jointly minimizing for some $\alpha$:
    \begin{align} \label{eq:aeloss}
      \sum_{(s, a, s')\in\mcD} \norm{d(e(s)) - s} + \alpha \norm{f(e(s), a) -
    e(s')}
    \end{align}
  \item
    Train a policy $\pi$ on $e$-encoded states using some reinforcement
    learning algorithm
\end{enumerate}
We plan on approaching the problem incrementally in the following way:
\begin{itemize}
  \item
    Hand-code $e,f$ for Pong (directly expose paddle and ball location and speed information). Verify improved DQN or PG performance.
  \item
    As above, but hard-code only $f$ (assuming $e$ extracts the appropriate information), then train $(e,d)$ to actually extract that information for the fixed $f$.
  \item
    As above, but jointly learn the autoencoder $(e, d)$ and the dynamics $f$.
  \item
    Apply the technique to other problems, besides Pong.
  \item
    Compare the performance of our approach to other existing approaches.
\end{itemize}

\section*{Related Work}

A variety of existing work has already explored similar approaches. We distinguish ourselves mainly by our autoencoder loss. First, the expression of the loss in Eq.~\ref{eq:aeloss} makes it a good fit for ladder networks \cite{rasmus2015semi}. We are unaware of prior work using ladder networks for auxiliary self-supervised objectives in RL. Second, we believe our loss will improve upon the modest improvements in prior PSR-for-RL work because its structure discourages several training degeneracies.

In \cite{munk2016learning}, states are encoded with loss $\norm{f(e(s)) - e(s')}$, which permits the minimizer $f(\cdot)=e(\cdot) = d(\cdot) = 1$. In \cite{watter2015embed}, the authors use locally linear dynamics on the latent space optimized jointly with encoding and decoding, and then solve a stochastic optimal control problem in latent space with iLQR to get an action plan. This is a pretty good use of the learned model; however, we believe that in practice it may be difficult to find environments where there are no locally linear dynamics. \cite{venkatraman2017predictive} lets $f$ be an RNN, whose state is used to predict statistics $\varphi$ about future observations, i.e. $\|d(f(e(s), a)) - \varphi(s')\|$. We view our model as simpler and possibly more applicable across tasks, and (via $\alpha$) exposing the tradeoff between encoding reconstruction accuracy and predictive power. Finally, \cite{shelhamer2016loss} is perhaps the most successful in this area, but their dynamics model is based on discriminating corrupted inputs rather than predicting states.

\bibliographystyle{plain}
\bibliography{citations}
\end{document}
